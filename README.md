# Delfos - Teste T√©cnico

Este projeto configura uma infraestrutura completa de dados, simulando um ambiente real com banco fonte, API de consumo e banco alvo para analytics.

## Como Rodar o Projeto

### Pr√©-requisitos
- Docker e Docker Compose
- Python 3.10+
- Pip

### 1. Subir a Infraestrutura
O projeto utiliza Docker Compose para orquestrar dois bancos PostgreSQL (`db_fonte` e `db_alvo`) e a API de dados (`api_conector`).

```bash
# Na raiz do projeto
docker compose up -d --build
```
Isso iniciar√°:
- **db_source**: Porta externa 5433
- **db_target**: Porta externa 5434
- **api_conector**: Porta externa 8000

### 2. Popular o Banco Fonte

#### Via Docker
Execute o script de setup diretamente pelo container da API (sem necessidade de instalar Python/libs na sua m√°quina):

```bash
docker compose exec api_conector python scripts/init_db_fonte.py
```

#### Via Python
Caso prefira rodar localmente:

```bash
pip install -r requirements.txt
python scripts/init_db_fonte.py
```

### 3. Executar o ETL
O script de ETL extrai dados da API, transforma e carrega no banco alvo.
**Observa√ß√£o**: Ao rodar localmente (fora do Docker), √© necess√°rio sobrescrever o host do banco alvo para `localhost`.

```bash
# Exemplo para rodar dados de uma data espec√≠fica
DB_TARGET_HOST=localhost python3 -m etl.main --date 2025-12-25
```

### 4. Orquestra√ß√£o com Dagster
O Dagster √© utilizado para orquestrar o processo de ETL diariamente, oferecendo interface visual, backfill e monitoramento.

1.  **Iniciar o Dagster**:
    ```bash
    dagster dev -m orchestrator
    ```

2.  **Acessar a UI**:
    Abra o navegador em [http://localhost:3000](http://localhost:3000).

3.  **Executar Job**:
    - V√° at√© a aba "Assets".
    - Clique em `daily_wind_etl`.
    - Clique em "Materialize" e escolha uma data de parti√ß√£o (lembre-se que o script de init gera dados apenas para os √∫ltimos 10 dias).

4.  **Agendamento**:
    O job est√° configurado para rodar diariamente √† meia-noite (UTC). Ative o schedule na aba "Overview" > "Schedules".

---

## Per√≠odo de Dados

O script de inicializa√ß√£o (`scripts/init_db_fonte.py`) gera dados retroativos de **10 dias** a partir do momento da sua execu√ß√£o.
- **Frequ√™ncia**: 1 minuto.
- **Volume**: ~14.400 registros.
- **Vari√°veis**: `timestamp`, `wind_speed` (simulado com aleatoriedade), `power` (curva de pot√™ncia baseada no vento) e `ambient_temperature`.

---

## API de Dados

O projeto exp√µe uma API RESTful (`api_conector`) desenvolvida com **FastAPI** para consultar os dados brutos do banco fonte.

### Endpoint: `GET /data`

Retorna uma lista de registros em formato JSON.

#### Par√¢metros de Consulta:
- `start_date` (opcional): Filtra registros a partir desta data/hora (ISO 8601).
- `end_date` (opcional): Filtra registros at√© esta data/hora (ISO 8601).
- `columns` (opcional): Lista separada por v√≠rgulas das colunas desejadas. Permite otimizar a transfer√™ncia de dados selecionando apenas o necess√°rio (ex: `wind_speed,power`).

#### Exemplo de Uso:
```bash
# Busca apenas velocidade do ar e pot√™ncia para um intervalo espec√≠fico
curl "http://localhost:8000/data?start_date=2025-12-25T00:00:00&end_date=2025-12-25T01:00:00&columns=wind_speed,power"
```

A API roda dentro do Docker e √© acess√≠vel internamente pelos servi√ßos (como o script ETL) via `http://api_conector:8000`.

---

## üèó Decis√µes de Design

### 1. Modelagem do Banco Alvo (Analytics)
Optou-se por uma modelagem **Vertical (EAV-like/Tabela Fato de Sinais)** em vez de uma tabela larga (wide).

- **Tabela `Signal`**: Armazena os metadados das vari√°veis (ex: `wind_speed_mean`, `power_max`).
    - *Vantagem*: Flexibilidade. Novos KPIs ou sensores podem ser adicionados sem alterar o esquema da tabela de dados (DDL).
- **Tabela `Data`**: Tabela fato contendo `timestamp`, `signal_id` e `value`.
    - *Vantagem*: Otimiza o armazenamento para s√©ries temporais esparsas e normaliza a estrutura de consulta.

### 2. ETL e Agrega√ß√£o
- **Extra√ß√£o**: Feita via API (`api_conector`) para simular um cen√°rio real de desacoplamento entre a fonte de dados (talvez um SCADA legado) e o pipeline de dados. O uso de `httpx` garante performance.
- **Transforma√ß√£o (Pandas)**:
    - **Janelamento**: Foi utilizado `resample('10min')` para reduzir a granularidade e suavizar ru√≠dos.
    - **M√©tricas**: Para cada janela de 10 minutos, calculamos estat√≠sticas descritivas (`mean`, `min`, `max`, `std`) que s√£o fundamentais para an√°lise de performance de ativos de energia.
    - **Flattening**: O DataFrame, originalmente "largo" ap√≥s a agrega√ß√£o, √© transformado para o formato "longo" para se adequar ao modelo de dados de destino.

---

## üìÇ Estrutura de Pastas

- `/api`: C√≥digo da aplica√ß√£o FastAPI.
- `/etl`: L√≥gica do pipeline (Extract, Transform, Load).
- `/scripts`: Scripts auxiliares de setup (infraestrutura).
- `/orchestrator`: Orquestra√ß√£o com Dagster.
- `docker-compose.yml`: Orquestra√ß√£o dos servi√ßos.
